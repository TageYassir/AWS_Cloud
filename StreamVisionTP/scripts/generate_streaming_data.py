"""
generate_streaming_data.py
Script de g√©n√©ration de donn√©es r√©alistes pour StreamVision
Auteur : √âquipe Data Engineering
Date : 2025-11-27
"""

import os
import sys
import random
import json
import logging
import io
from datetime import datetime, timedelta, date, time
from decimal import Decimal
from typing import List, Dict, Tuple, Any

import psycopg2
from psycopg2.extras import execute_batch, RealDictCursor
import pandas as pd
import numpy as np
from faker import Faker
from tqdm import tqdm

# ============================================================================
# LOGGING (UTF-8 safe pour la console Windows)
# ============================================================================
formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')

# File handler √©crit en UTF-8 (pour stocker les emojis dans le fichier de log sans erreur)
file_handler = logging.FileHandler('data_generation.log', encoding='utf-8')
file_handler.setFormatter(formatter)

# Stream handler: sur Windows la console utilise souvent cp1252 ‚Äî on la wrap en UTF-8
# pour √©viter UnicodeEncodeError lors des messages contenant des emojis.
try:
    stream = sys.stdout
    if hasattr(sys.stdout, "buffer"):
        # TextIOWrapper autour de stdout.buffer permet de forcer l'encodage en UTF-8
        stream = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace', line_buffering=True)
except Exception:
    stream = sys.stdout

stream_handler = logging.StreamHandler(stream)
stream_handler.setFormatter(formatter)

logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)
# Remove any existing handlers to avoid duplication in interactive re-runs
if logger.hasHandlers():
    logger.handlers.clear()
logger.addHandler(file_handler)
logger.addHandler(stream_handler)

# ============================================================================
# CONFIGURATION
# ============================================================================

# Configuration PostgreSQL - √Ä MODIFIER SELON VOTRE INSTALLATION
DB_CONFIG = {
    'host': 'localhost',
    'port': 5432,
    'database': 'streamvision',
    'user': 'postgres',
    'password': '1234'  # ‚ö†Ô∏è METTEZ VOTRE MOT DE PASSE ICI
}

# Constantes m√©tier
COUNTRIES = ['FRA', 'USA', 'DEU', 'ESP', 'GBR', 'ITA', 'CAN', 'AUS', 'JPN', 'KOR', 'BRA', 'MEX', 'IND', 'CHN']
AGE_GROUPS = ['13-17', '18-24', '25-34', '35-44', '45-54', '55+']
SUBSCRIPTION_PLANS = ['free_trial', 'basic', 'standard', 'premium', 'family']
CONTENT_TYPES = ['movie', 'tv_show', 'documentary', 'short_film', 'original']
GENRES = [
    'Action', 'Adventure', 'Animation', 'Comedy', 'Crime', 'Documentary',
    'Drama', 'Family', 'Fantasy', 'History', 'Horror', 'Music', 'Mystery',
    'Romance', 'Science Fiction', 'Thriller', 'War', 'Western'
]
PLATFORMS = ['web', 'mobile_ios', 'mobile_android', 'smart_tv', 'game_console', 'tablet']
DEVICE_TYPES = ['desktop', 'laptop', 'phone', 'tablet', 'tv', 'console']
QUALITIES = ['SD', 'HD', 'Full HD', '4K', 'HDR']
SUBSCRIPTION_EVENTS = ['subscription_start', 'upgrade', 'downgrade', 'cancellation', 'renewal', 'payment_failed']

# Initialisation de Faker avec plusieurs langues
fake = Faker(['fr_FR', 'en_US', 'de_DE', 'es_ES', 'it_IT', 'pt_BR', 'ja_JP', 'ko_KR'])


# ============================================================================
# UTILITAIRES
# ============================================================================

def to_datetime(d):
    """Convertit date -> datetime (00:00:00). Si d√©j√† datetime, renvoie tel quel."""
    if isinstance(d, datetime):
        return d
    if isinstance(d, date):
        return datetime.combine(d, time.min)
    return d


def get_db_connection():
    """√âtablit une connexion √† PostgreSQL"""
    try:
        conn = psycopg2.connect(**DB_CONFIG)
        conn.autocommit = False
        logger.info("Connexion PostgreSQL √©tablie avec succ√®s")
        return conn
    except Exception as e:
        logger.error(f"Erreur de connexion PostgreSQL: {e}")
        sys.exit(1)

# ============================================================================
# üö® FLUSH DATABASE (SUPPRESSION TOTALE DES DONN√âES)
# ============================================================================

def flush_database(conn):
    """
    Supprime TOUTES les donn√©es de la base StreamVision.
    Action IRR√âVERSIBLE.
    """
    print("\n" + "‚ö†Ô∏è" * 30)
    print("‚ö†Ô∏è  DANGER ‚Äì FLUSH DE LA BASE DE DONN√âES")
    print("‚ö†Ô∏è  Cette action SUPPRIME TOUTES LES DONN√âES.")
    print("‚ö†Ô∏è  Elle est IRR√âVERSIBLE.")
    print("‚ö†Ô∏è" * 30)

    confirm = input("\nTapez exactement 'YES' pour confirmer : ").strip()

    if confirm != "YES":
        print("‚ùå Op√©ration annul√©e.")
        return

    tables = [
        "episode_viewing",
        "episodes",
        "search_queries",
        "subscription_events",
        "watchlist",
        "ratings",
        "viewing_sessions",
        "content",
        "users"
    ]

    cursor = conn.cursor()

    try:
        logger.warning("FLUSH DATABASE INITI√â")

        cursor.execute(
            f"TRUNCATE TABLE {', '.join(tables)} RESTART IDENTITY CASCADE;"
        )

        conn.commit()

        logger.warning("üî• BASE DE DONN√âES COMPL√àTEMENT VID√âE")
        print("\nüßπ BASE DE DONN√âES FLUSH√âE AVEC SUCC√àS")

    except Exception as e:
        conn.rollback()
        logger.error("Erreur lors du flush de la base", exc_info=True)
        print(f"\n‚ùå ERREUR LORS DU FLUSH : {e}")

    finally:
        cursor.close()



def generate_realistic_movie_titles(count: int = 100) -> List[str]:
    """G√©n√®re des titres de films r√©alistes"""
    titles = []
    prefixes = ['The', 'A', 'My', 'Our', 'Your', 'His', 'Her', 'Their']
    adjectives = ['Last', 'First', 'Great', 'Big', 'Small', 'Lost', 'Found', 'Hidden', 'Secret']
    nouns = ['Journey', 'Adventure', 'Dream', 'Night', 'Day', 'Love', 'War', 'Peace', 'Hope']
    suffixes = ['Returns', 'Begins', 'Ends', 'Rises', 'Falls', 'Lives', 'Dies']

    for _ in range(count):
        r = random.random()
        if r < 0.3:
            title = f"{random.choice(adjectives)} {random.choice(nouns)}"
        elif r < 0.6:
            title = f"{random.choice(prefixes)} {random.choice(adjectives)} {random.choice(nouns)}"
        else:
            title = f"{random.choice(nouns)} of the {random.choice(nouns)}"

        if random.random() < 0.2:
            title = f"{title}: {random.choice(suffixes)}"

        titles.append(title)

    return titles


def calculate_age_group(birth_year: int) -> str:
    """Calcule le groupe d'√¢ge bas√© sur l'ann√©e de naissance"""
    current_year = datetime.now().year
    age = current_year - birth_year

    if age < 13:
        return '13-17'  # Pour la d√©mo, on consid√®re que tous ont au moins 13 ans
    elif age <= 17:
        return '13-17'
    elif age <= 24:
        return '18-24'
    elif age <= 34:
        return '25-34'
    elif age <= 44:
        return '35-44'
    elif age <= 54:
        return '45-54'
    else:
        return '55+'


# ============================================================================
# G√âN√âRATION DES DONN√âES
# ============================================================================

def generate_users(conn, n_users: int = 10000):
    """G√©n√®re des utilisateurs r√©alistes pour une plateforme de streaming"""
    logger.info(f"D√©but de la g√©n√©ration de {n_users} utilisateurs...")

    cursor = conn.cursor()
    users_data = []

    # Titres de films r√©alistes pour les usernames (non utilis√©s directement mais pr√™ts)
    movie_titles = generate_realistic_movie_titles(500)

    # Progress bar
    pbar = tqdm(total=n_users, desc="G√©n√©ration des utilisateurs", unit="user")

    for i in range(n_users):
        # Informations de base
        email = fake.unique.email()
        username = f"{fake.user_name()}_{random.randint(100, 999)}"
        first_name = fake.first_name()
        last_name = fake.last_name()
        country = random.choice(COUNTRIES)

        # Calcul de l'√¢ge
        birth_year = random.randint(1950, 2010)
        age_group = calculate_age_group(birth_year)

        # Abonnement
        subscription_plan = random.choice(SUBSCRIPTION_PLANS)

        # Dates d'abonnement (√©viter les cha√Ænes de parse de faker en passant des objets date)
        subscription_start = fake.date_between(
            start_date=date.today() - timedelta(days=365),
            end_date=date.today()
        )

        if subscription_plan == 'free_trial':
            # Essai gratuit de 7 √† 30 jours
            subscription_end = subscription_start + timedelta(days=random.randint(7, 30))
        else:
            # Abonnement payant de 1 mois √† 2 ans
            subscription_end = subscription_start + timedelta(days=random.randint(30, 730))

        # Dates de cr√©ation et derni√®re connexion
        created_at = fake.date_time_between(
            start_date=to_datetime(subscription_start - timedelta(days=7)),
            end_date=to_datetime(subscription_start)
        )

        # Derni√®re connexion (plus r√©cente que la cr√©ation)
        if random.random() < 0.8:  # 80% des utilisateurs se sont connect√©s r√©cemment
            last_login = fake.date_time_between(
                start_date=datetime.now() - timedelta(days=30),
                end_date=datetime.now()
            )
        else:
            last_login = None  # Utilisateurs inactifs

        # Statut actif
        is_active = random.random() < 0.85  # 85% d'utilisateurs actifs

        # M√©thode de paiement
        payment_methods = ['credit_card', 'paypal', 'apple_pay', 'google_pay', 'bank_transfer']
        payment_method = random.choice(payment_methods) if subscription_plan != 'free_trial' else None

        # Pr√©f√©rence d'appareil
        device_preference = random.choice(DEVICE_TYPES)

        user_record = (
            email, username, first_name, last_name, country, age_group,
            subscription_plan, subscription_start, subscription_end,
            created_at, last_login, is_active, payment_method, device_preference
        )

        users_data.append(user_record)

        # Insertion par lots de 1000
        if len(users_data) >= 1000:
            query = """
                INSERT INTO users (
                    email, username, first_name, last_name, country, age_group,
                    subscription_plan, subscription_start, subscription_end,
                    created_at, last_login, is_active, payment_method, device_preference
                ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
            """
            execute_batch(cursor, query, users_data)
            users_data = []

        pbar.update(1)

    # Insertion des donn√©es restantes
    if users_data:
        query = """
            INSERT INTO users (
                email, username, first_name, last_name, country, age_group,
                subscription_plan, subscription_start, subscription_end,
                created_at, last_login, is_active, payment_method, device_preference
            ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
        """
        execute_batch(cursor, query, users_data)

    pbar.close()
    conn.commit()
    cursor.close()
    logger.info(f"‚úÖ {n_users} utilisateurs g√©n√©r√©s avec succ√®s")


def generate_content(conn, n_content: int = 5000):
    """G√©n√®re du contenu vid√©o (films, s√©ries, documentaires)"""
    logger.info(f"D√©but de la g√©n√©ration de {n_content} contenus...")

    cursor = conn.cursor()
    content_data = []

    # Titres de films r√©alistes
    movie_titles = generate_realistic_movie_titles(n_content + 1000)

    # Noms de r√©alisateurs c√©l√®bres (fictifs pour la d√©mo)
    directors = [
        'Christopher Nolan', 'Steven Spielberg', 'Martin Scorsese', 'Quentin Tarantino',
        'James Cameron', 'David Fincher', 'Ridley Scott', 'Tim Burton', 'Wes Anderson',
        'Alfred Hitchcock', 'Stanley Kubrick', 'Francis Ford Coppola', 'George Lucas',
        'Peter Jackson', 'Guillermo del Toro', 'Hayao Miyazaki', 'Bong Joon-ho',
        'Denis Villeneuve', 'Ava DuVernay', 'Greta Gerwig', 'Jordan Peele'
    ]

    # Acteurs principaux
    actors = [
        'Leonardo DiCaprio', 'Meryl Streep', 'Tom Hanks', 'Denzel Washington',
        'Jennifer Lawrence', 'Robert Downey Jr.', 'Scarlett Johansson', 'Brad Pitt',
        'Angelina Jolie', 'Johnny Depp', 'Emma Stone', 'Ryan Gosling', 'Margot Robbie',
        'Will Smith', 'Natalie Portman', 'Christian Bale', 'Anne Hathaway', 'Matt Damon',
        'Cate Blanchett', 'Joaquin Phoenix', 'Viola Davis', 'Samuel L. Jackson',
        'Morgan Freeman', 'Keanu Reeves', 'Charlize Theron'
    ]

    # Progress bar
    pbar = tqdm(total=n_content, desc="G√©n√©ration des contenus", unit="content")

    for i in range(n_content):
        # Titre
        title = movie_titles[i]

        # Type de contenu
        content_type = random.choice(CONTENT_TYPES)

        # Genres (1 √† 3 genres par contenu)
        if content_type == 'documentary':
            main_genre = 'Documentary'
            subgenres = random.sample([g for g in GENRES if g != 'Documentary'], random.randint(0, 2))
        else:
            main_genre = random.choice([g for g in GENRES if g != 'Documentary'])
            subgenres = random.sample([g for g in GENRES if g not in [main_genre, 'Documentary']], random.randint(0, 2))

        genre = main_genre
        subgenre = ', '.join(subgenres) if subgenres else None

        # Ann√©e de sortie
        if content_type == 'tv_show':
            release_year = random.randint(1990, 2024)
        else:
            release_year = random.randint(1970, 2024)

        # Dur√©e selon le type
        if content_type == 'movie':
            duration = random.randint(80, 180)  # 1h20 √† 3h
        elif content_type == 'tv_show':
            duration = random.randint(40, 60)  # Dur√©e par √©pisode
        elif content_type == 'documentary':
            duration = random.randint(50, 120)  # 50min √† 2h
        else:  # short_film ou original
            duration = random.randint(10, 50)

        # R√©alisateur et acteur
        director = random.choice(directors)
        main_actor = random.choice(actors)

        # Note IMDB r√©aliste (distribution normale)
        imdb_mean = 7.0 if content_type == 'original' else 6.5
        imdb_std = 1.5
        imdb_rating = np.random.normal(imdb_mean, imdb_std)
        imdb_rating = max(1.0, min(10.0, round(imdb_rating, 1)))  # Born√© entre 1 et 10

        # Classification
        content_ratings = ['G', 'PG', 'PG-13', 'R', 'NC-17']
        weights = [0.1, 0.2, 0.4, 0.25, 0.05]
        content_rating = random.choices(content_ratings, weights=weights)[0]

        # Contenu original (20% de chance)
        is_original = random.random() < 0.2

        # Date d'ajout √† la plateforme (passer des objets date)
        added_date = fake.date_between(
            start_date=date(release_year, 1, 1),
            end_date=date.today()
        )

        # Pays disponibles (3 √† 10 pays al√©atoires)
        available_countries = random.sample(COUNTRIES, random.randint(3, 10))

        # Tags (mots-cl√©s)
        tags = []
        if main_genre:
            tags.append(main_genre.lower())
        if subgenres:
            tags.extend([g.lower() for g in subgenres])
        tags.extend(random.sample(['popular', 'new', 'award', 'oscar', 'bestseller', 'trending'], 2))

        # Description
        description = fake.text(max_nb_chars=200)

        content_record = (
            title, content_type, genre, subgenre, release_year, duration,
            director, main_actor, float(imdb_rating), content_rating,
            is_original, added_date, available_countries, tags, description
        )

        content_data.append(content_record)

        # Insertion par lots de 500
        if len(content_data) >= 500:
            query = """
                INSERT INTO content (
                    title, content_type, genre, subgenre, release_year, duration_minutes,
                    director, main_actor, imdb_rating, content_rating, is_original,
                    added_date, available_countries, tags, description
                ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
            """
            execute_batch(cursor, query, content_data)
            content_data = []

        pbar.update(1)

    # Insertion des donn√©es restantes
    if content_data:
        query = """
            INSERT INTO content (
                title, content_type, genre, subgenre, release_year, duration_minutes,
                director, main_actor, imdb_rating, content_rating, is_original,
                added_date, available_countries, tags, description
            ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
        """
        execute_batch(cursor, query, content_data)

    pbar.close()
    conn.commit()
    cursor.close()
    logger.info(f"‚úÖ {n_content} contenus g√©n√©r√©s avec succ√®s")


def generate_viewing_sessions(conn, n_sessions: int = 100000):
    """G√©n√®re des sessions de visionnage r√©alistes"""
    logger.info(f"D√©but de la g√©n√©ration de {n_sessions} sessions de visionnage...")

    cursor = conn.cursor()

    # R√©cup√©ration des IDs utilisateurs (actifs seulement)
    cursor.execute("SELECT id FROM users WHERE is_active = TRUE LIMIT 5000")
    user_ids = [row[0] for row in cursor.fetchall()]

    # R√©cup√©ration des IDs de contenu
    cursor.execute("SELECT id, duration_minutes FROM content")
    content_data = cursor.fetchall()

    if not user_ids or not content_data:
        logger.error("Pas d'utilisateurs ou de contenus. G√©n√©rez-les d'abord.")
        return

    sessions_data = []

    # Progress bar
    pbar = tqdm(total=n_sessions, desc="G√©n√©ration des sessions", unit="session")

    for i in range(n_sessions):
        # Utilisateur al√©atoire
        user_id = random.choice(user_ids)

        # Contenu al√©atoire
        content_id, content_duration = random.choice(content_data)

        # Date de la session (derniers 6 mois) - utiliser des datetimes explicites
        session_start = fake.date_time_between(
            start_date=datetime.now() - timedelta(days=180),
            end_date=datetime.now()
        )

        # Dur√©e de visionnage (entre 5 minutes et la dur√©e totale du contenu)
        max_watch_seconds = min(content_duration * 60, 7200)  # Max 2 heures
        min_watch_seconds = 300  # Min 5 minutes

        duration_seconds = random.randint(min_watch_seconds, max_watch_seconds)

        # Heure de fin
        session_end = session_start + timedelta(seconds=duration_seconds)

        # Taux de compl√©tion
        completion_rate = min(duration_seconds / (content_duration * 60), 1.0) * 100

        # Plateforme et appareil
        platform = random.choice(PLATFORMS)

        # Mapping plateforme -> appareil
        platform_device_map = {
            'web': ['desktop', 'laptop'],
            'mobile_ios': ['phone', 'tablet'],
            'mobile_android': ['phone', 'tablet'],
            'smart_tv': ['tv'],
            'game_console': ['console'],
            'tablet': ['tablet']
        }
        device_type = random.choice(platform_device_map[platform])

        # Qualit√© (plus probable pour les appareils r√©cents)
        if device_type in ['tv', 'desktop', 'laptop']:
            quality_weights = [0.1, 0.3, 0.4, 0.15, 0.05]  # Plus de Full HD
        else:
            quality_weights = [0.2, 0.5, 0.2, 0.05, 0.05]  # Plus de HD sur mobile

        quality = random.choices(QUALITIES, weights=quality_weights)[0]

        # Nombre de buffering (0-5)
        buffering_count = random.randint(0, 5)

        # Bitrate moyen (en kbps)
        quality_bitrate = {
            'SD': 1000,
            'HD': 3000,
            'Full HD': 6000,
            '4K': 15000,
            'HDR': 20000
        }
        avg_bitrate = quality_bitrate[quality] + random.randint(-500, 500)

        # Ville (optionnelle, 70% de chances)
        if random.random() < 0.7:
            city = fake.city()
        else:
            city = None

        # Adresse IP (optionnelle)
        ip_address = fake.ipv4() if random.random() < 0.5 else None

        session_record = (
            user_id, content_id, session_start, session_end, duration_seconds,
            platform, device_type, quality, round(completion_rate, 2),
            buffering_count, avg_bitrate, city, ip_address
        )

        sessions_data.append(session_record)

        # Insertion par lots de 2000
        if len(sessions_data) >= 2000:
            query = """
                INSERT INTO viewing_sessions (
                    user_id, content_id, session_start, session_end, duration_seconds,
                    platform, device_type, quality, completion_rate, buffering_count,
                    avg_bitrate, city, ip_address
                ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
            """
            execute_batch(cursor, query, sessions_data)
            sessions_data = []

        pbar.update(1)

    # Insertion des donn√©es restantes
    if sessions_data:
        query = """
            INSERT INTO viewing_sessions (
                user_id, content_id, session_start, session_end, duration_seconds,
                platform, device_type, quality, completion_rate, buffering_count,
                avg_bitrate, city, ip_address
            ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
        """
        execute_batch(cursor, query, sessions_data)

    pbar.close()
    conn.commit()
    cursor.close()
    logger.info(f"‚úÖ {n_sessions} sessions de visionnage g√©n√©r√©es avec succ√®s")


def generate_ratings(conn, n_ratings: int = 30000):
    """G√©n√®re des √©valuations r√©alistes"""
    logger.info(f"D√©but de la g√©n√©ration de {n_ratings} √©valuations...")

    cursor = conn.cursor()

    # R√©cup√©ration des IDs utilisateurs (qui ont regard√© du contenu)
    cursor.execute("""
        SELECT DISTINCT user_id 
        FROM viewing_sessions 
        WHERE user_id IS NOT NULL 
        LIMIT 3000
    """)
    user_ids = [row[0] for row in cursor.fetchall()]

    # R√©cup√©ration des IDs de contenu
    cursor.execute("SELECT id FROM content LIMIT 1000")
    content_ids = [row[0] for row in cursor.fetchall()]

    ratings_data = []

    # Progress bar
    pbar = tqdm(total=n_ratings, desc="G√©n√©ration des √©valuations", unit="rating")

    # Garder une trace des paires (user_id, content_id) pour √©viter les doublons
    user_content_pairs = set()

    for i in range(n_ratings):
        # S√©lectionner un utilisateur et un contenu
        user_id = random.choice(user_ids)
        content_id = random.choice(content_ids)

        # V√©rifier si cette paire existe d√©j√†
        pair = (user_id, content_id)
        if pair in user_content_pairs:
            continue  # Passer √† l'it√©ration suivante

        user_content_pairs.add(pair)

        # Note (distribution normale centr√©e sur 3.5/5)
        rating_raw = np.random.normal(3.5, 1.0)
        rating = max(1, min(5, round(rating_raw)))

        # Date d'√©valuation (apr√®s la date d'ajout du contenu) - utiliser des datetimes explicites
        rating_date = fake.date_time_between(
            start_date=datetime.now() - timedelta(days=180),
            end_date=datetime.now()
        )

        # Texte de critique (30% de chances)
        if random.random() < 0.3:
            review_text = fake.text(max_nb_chars=200)
        else:
            review_text = None

        # Nombre de "utile" (0-50)
        helpful_count = random.randint(0, 50)

        rating_record = (
            user_id, content_id, rating, rating_date, review_text, helpful_count
        )

        ratings_data.append(rating_record)

        # Insertion par lots de 1000
        if len(ratings_data) >= 1000:
            query = """
                INSERT INTO ratings (
                    user_id, content_id, rating, rating_date, review_text, helpful_count
                ) VALUES (%s, %s, %s, %s, %s, %s)
            """
            execute_batch(cursor, query, ratings_data)
            ratings_data = []

        pbar.update(1)

    # Insertion des donn√©es restantes
    if ratings_data:
        query = """
            INSERT INTO ratings (
                user_id, content_id, rating, rating_date, review_text, helpful_count
            ) VALUES (%s, %s, %s, %s, %s, %s)
        """
        execute_batch(cursor, query, ratings_data)

    pbar.close()
    conn.commit()
    cursor.close()
    logger.info(f"‚úÖ {n_ratings} √©valuations g√©n√©r√©es avec succ√®s")


def generate_watchlist(conn, n_items: int = 20000):
    """G√©n√®re des entr√©es de liste de visionnage"""
    logger.info(f"D√©but de la g√©n√©ration de {n_items} entr√©es de liste de visionnage...")

    cursor = conn.cursor()

    # R√©cup√©ration des IDs utilisateurs
    cursor.execute("SELECT id FROM users WHERE is_active = TRUE LIMIT 3000")
    user_ids = [row[0] for row in cursor.fetchall()]

    # R√©cup√©ration des IDs de contenu
    cursor.execute("SELECT id FROM content LIMIT 1500")
    content_ids = [row[0] for row in cursor.fetchall()]

    watchlist_data = []

    # Progress bar
    pbar = tqdm(total=n_items, desc="G√©n√©ration de la watchlist", unit="item")

    # Garder une trace des paires (user_id, content_id)
    user_content_pairs = set()

    for i in range(n_items):
        user_id = random.choice(user_ids)
        content_id = random.choice(content_ids)

        # V√©rifier si cette paire existe d√©j√†
        pair = (user_id, content_id)
        if pair in user_content_pairs:
            continue

        user_content_pairs.add(pair)

        # Date d'ajout
        added_date = fake.date_time_between(
            start_date=datetime.now() - timedelta(days=90),
            end_date=datetime.now()
        )

        # Statut "regard√©" (40% de chances)
        watched = random.random() < 0.4

        if watched:
            watched_date = added_date + timedelta(days=random.randint(1, 30))
        else:
            watched_date = None

        watchlist_record = (
            user_id, content_id, added_date, watched, watched_date
        )

        watchlist_data.append(watchlist_record)

        # Insertion par lots de 1000
        if len(watchlist_data) >= 1000:
            query = """
                INSERT INTO watchlist (
                    user_id, content_id, added_date, watched, watched_date
                ) VALUES (%s, %s, %s, %s, %s)
            """
            execute_batch(cursor, query, watchlist_data)
            watchlist_data = []

        pbar.update(1)

    # Insertion des donn√©es restantes
    if watchlist_data:
        query = """
            INSERT INTO watchlist (
                user_id, content_id, added_date, watched, watched_date
            ) VALUES (%s, %s, %s, %s, %s)
        """
        execute_batch(cursor, query, watchlist_data)

    pbar.close()
    conn.commit()
    cursor.close()
    logger.info(f"‚úÖ {n_items} entr√©es de watchlist g√©n√©r√©es avec succ√®s")


def generate_subscription_events(conn, n_events: int = 15000):
    """G√©n√®re des √©v√©nements d'abonnement"""
    logger.info(f"D√©but de la g√©n√©ration de {n_events} √©v√©nements d'abonnement...")

    cursor = conn.cursor()

    # R√©cup√©ration des IDs utilisateurs
    cursor.execute("SELECT id, subscription_plan FROM users LIMIT 4000")
    users = cursor.fetchall()

    events_data = []

    # Progress bar
    pbar = tqdm(total=n_events, desc="G√©n√©ration des √©v√©nements d'abonnement", unit="event")

    # Garde-fou pour √©viter trop d'√©v√©nements par utilisateur
    user_event_count = {}

    for i in range(n_events):
        user_id, current_plan = random.choice(users)

        # Limiter √† 5 √©v√©nements par utilisateur
        if user_id not in user_event_count:
            user_event_count[user_id] = 0

        if user_event_count[user_id] >= 5:
            continue

        user_event_count[user_id] += 1

        # Type d'√©v√©nement
        if current_plan == 'free_trial':
            event_types = ['subscription_start', 'upgrade', 'cancellation']
            weights = [0.6, 0.3, 0.1]
        else:
            event_types = ['renewal', 'upgrade', 'downgrade', 'cancellation', 'payment_failed']
            weights = [0.5, 0.2, 0.1, 0.1, 0.1]

        event_type = random.choices(event_types, weights=weights)[0]

        # Date de l'√©v√©nement (dans les 2 derni√®res ann√©es)
        event_date = fake.date_time_between(
            start_date=datetime.now() - timedelta(days=730),
            end_date=datetime.now()
        )

        # Plans pr√©c√©dents et nouveaux
        if event_type == 'subscription_start':
            previous_plan = None
            new_plan = 'free_trial'
        elif event_type in ['upgrade', 'downgrade']:
            previous_plan = current_plan
            possible_plans = [p for p in SUBSCRIPTION_PLANS if p != current_plan]
            new_plan = random.choice(possible_plans)
        elif event_type == 'cancellation':
            previous_plan = current_plan
            new_plan = None
        else:  # renewal ou payment_failed
            previous_plan = current_plan
            new_plan = current_plan

        # Montant (selon le plan)
        plan_prices = {
            'free_trial': 0,
            'basic': 7.99,
            'standard': 10.99,
            'premium': 15.99,
            'family': 19.99
        }

        amount = None
        if new_plan and event_type != 'payment_failed':
            amount = plan_prices.get(new_plan, 0)
            amount = round(amount * random.uniform(0.9, 1.1), 2)

        # Devise
        currency = 'USD'

        # Passerelle de paiement
        payment_gateways = ['stripe', 'paypal', 'apple_pay', 'google_pay']
        payment_gateway = random.choice(payment_gateways) if amount and amount > 0 else None

        # ID de transaction (pour les paiements)
        transaction_id = None
        if amount and amount > 0:
            transaction_id = f"txn_{random.randint(100000000, 999999999)}"

        event_record = (
            user_id, event_type, event_date, previous_plan, new_plan,
            amount, currency, payment_gateway, transaction_id
        )

        events_data.append(event_record)

        # Insertion par lots de 1000
        if len(events_data) >= 1000:
            query = """
                INSERT INTO subscription_events (
                    user_id, event_type, event_date, previous_plan, new_plan,
                    amount, currency, payment_gateway, transaction_id
                ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s)
            """
            execute_batch(cursor, query, events_data)
            events_data = []

        pbar.update(1)

    # Insertion des donn√©es restantes
    if events_data:
        query = """
            INSERT INTO subscription_events (
                user_id, event_type, event_date, previous_plan, new_plan,
                amount, currency, payment_gateway, transaction_id
            ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s)
        """
        execute_batch(cursor, query, events_data)

    pbar.close()
    conn.commit()
    cursor.close()
    logger.info(f"‚úÖ {n_events} √©v√©nements d'abonnement g√©n√©r√©s avec succ√®s")


def generate_search_queries(conn, n_queries: int = 25000):
    """G√©n√®re des requ√™tes de recherche"""
    logger.info(f"D√©but de la g√©n√©ration de {n_queries} requ√™tes de recherche...")

    cursor = conn.cursor()

    # R√©cup√©ration des IDs utilisateurs
    cursor.execute("SELECT id FROM users WHERE is_active = TRUE LIMIT 3000")
    user_ids = [row[0] for row in cursor.fetchall()]

    # R√©cup√©ration des IDs de contenu
    cursor.execute("SELECT id, title, genre FROM content LIMIT 1000")
    content_data = cursor.fetchall()

    # Mots-cl√©s de recherche populaires
    search_keywords = [
        'action', 'comedy', 'drama', 'horror', 'romance', 'sci-fi', 'thriller',
        'documentary', 'animation', 'fantasy', 'adventure', 'crime', 'mystery',
        'new', 'popular', 'trending', 'oscar', 'award', 'best', 'top',
        'movie', 'film', 'series', 'show', 'tv', 'netflix', 'amazon',
        '2023', '2024', '2022', '2021', '2020',
        'christmas', 'halloween', 'summer', 'winter',
        'kid', 'family', 'adult', 'teen'
    ]

    queries_data = []

    # Progress bar
    pbar = tqdm(total=n_queries, desc="G√©n√©ration des recherches", unit="query")

    for i in range(n_queries):
        # Utilisateur (70% de chances d'√™tre connect√©)
        if random.random() < 0.7 and user_ids:
            user_id = random.choice(user_ids)
        else:
            user_id = None

        # Texte de recherche
        r = random.random()
        if r < 0.3 and content_data:
            # Recherche par titre
            _, title, _ = random.choice(content_data)
            query_text = title
        elif r < 0.5:
            # Recherche par genre/mot-cl√©
            query_text = random.choice(search_keywords)
        else:
            # Recherche combin√©e
            words = random.sample(search_keywords, random.randint(1, 3))
            query_text = ' '.join(words)

        # Date de recherche
        search_date = fake.date_time_between(
            start_date=datetime.now() - timedelta(days=90),
            end_date=datetime.now()
        )

        # Nombre de r√©sultats
        results_count = random.randint(0, 200)

        # Contenu cliqu√© (30% de chances)
        clicked_content_id = None
        if random.random() < 0.3 and content_data:
            clicked_content_id = random.choice(content_data)[0]

        # Filtres de recherche (JSON)
        search_filters = None
        if random.random() < 0.4:
            filters = {}
            if random.random() < 0.5:
                filters['genre'] = random.choice(GENRES)
            if random.random() < 0.3:
                filters['year'] = random.randint(2000, 2024)
            if random.random() < 0.2:
                filters['rating'] = random.choice(['G', 'PG', 'PG-13', 'R'])
            if filters:
                search_filters = json.dumps(filters)

        # ID de session
        session_id = f"sess_{random.randint(100000, 999999)}"

        query_record = (
            user_id, query_text, search_date, results_count,
            clicked_content_id, search_filters, session_id
        )

        queries_data.append(query_record)

        # Insertion par lots de 1000
        if len(queries_data) >= 1000:
            query = """
                INSERT INTO search_queries (
                    user_id, query_text, search_date, results_count,
                    clicked_content_id, search_filters, session_id
                ) VALUES (%s, %s, %s, %s, %s, %s, %s)
            """
            execute_batch(cursor, query, queries_data)
            queries_data = []

        pbar.update(1)

    # Insertion des donn√©es restantes
    if queries_data:
        query = """
            INSERT INTO search_queries (
                user_id, query_text, search_date, results_count,
                clicked_content_id, search_filters, session_id
            ) VALUES (%s, %s, %s, %s, %s, %s, %s)
        """
        execute_batch(cursor, query, queries_data)

    pbar.close()
    conn.commit()
    cursor.close()
    logger.info(f"‚úÖ {n_queries} requ√™tes de recherche g√©n√©r√©es avec succ√®s")


def generate_episodes_and_viewing(conn, n_episodes: int = 5000, n_episode_views: int = 30000):
    """G√©n√®re des √©pisodes (pour les s√©ries) et leur visionnage"""
    logger.info(f"D√©but de la g√©n√©ration de {n_episodes} √©pisodes et {n_episode_views} visionnages d'√©pisodes...")

    cursor = conn.cursor()

    # R√©cup√©ration des s√©ries TV seulement
    cursor.execute("SELECT id, title FROM content WHERE content_type = 'tv_show' LIMIT 200")
    tv_shows = cursor.fetchall()

    if not tv_shows:
        logger.warning("Aucune s√©rie TV trouv√©e. G√©n√©ration de quelques s√©ries...")
        # Cr√©er quelques s√©ries TV manuellement
        cursor.execute("""
            INSERT INTO content (title, content_type, genre, release_year, duration_minutes)
            VALUES 
                ('The Adventure Chronicles', 'tv_show', 'Adventure', 2020, 45),
                ('Medical Dreams', 'tv_show', 'Drama', 2018, 50),
                ('Space Explorers', 'tv_show', 'Science Fiction', 2022, 60)
            RETURNING id, title
        """)
        tv_shows = cursor.fetchall()
        conn.commit()

    # ============================================================================
    # √âTAPE 1 : G√©n√©ration des √©pisodes
    # ============================================================================
    logger.info("G√©n√©ration des √©pisodes...")
    episodes_data = []

    for tv_show_id, tv_show_title in tv_shows:
        # Nombre de saisons (1 √† 5)
        n_seasons = random.randint(1, 5)

        for season in range(1, n_seasons + 1):
            # Nombre d'√©pisodes par saison (6 √† 13)
            n_episodes_per_season = random.randint(6, 13)

            for episode in range(1, n_episodes_per_season + 1):
                # Titre de l'√©pisode
                episode_titles = [
                    'Pilot', 'Beginnings', 'Endings', 'The Start', 'The Finish',
                    'Unexpected', 'Revelations', 'Secrets', 'Truth', 'Lies',
                    'Alliances', 'Betrayals', 'Hope', 'Despair', 'Love', 'Hate'
                ]
                episode_title = f"Episode {episode}: {random.choice(episode_titles)}"

                # Dur√©e (40-60 minutes)
                duration_minutes = random.randint(40, 60)

                # Date de sortie (derniers ~5 ans)
                release_date = fake.date_between(
                    start_date=date.today() - timedelta(days=5 * 365),
                    end_date=date.today()
                )

                # R√©alisateur
                director = fake.name()

                # Note IMDB
                imdb_rating = round(random.uniform(6.0, 9.5), 1)

                # Description
                description = fake.text(max_nb_chars=150)

                episode_record = (
                    tv_show_id, season, episode, episode_title, duration_minutes,
                    release_date, director, float(imdb_rating), description
                )

                episodes_data.append(episode_record)

                # Limiter le nombre total d'√©pisodes
                if len(episodes_data) >= n_episodes:
                    break

            if len(episodes_data) >= n_episodes:
                break

        if len(episodes_data) >= n_episodes:
            break

    # Insertion des √©pisodes
    if episodes_data:
        query = """
            INSERT INTO episodes (
                tv_show_id, season_number, episode_number, title, duration_minutes,
                release_date, director, imdb_rating, description
            ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s)
        """
        execute_batch(cursor, query, episodes_data)

    conn.commit()
    logger.info(f"‚úÖ {len(episodes_data)} √©pisodes g√©n√©r√©s avec succ√®s")

    # ============================================================================
    # √âTAPE 2 : G√©n√©ration des visionnages d'√©pisodes
    # ============================================================================
    logger.info("G√©n√©ration des visionnages d'√©pisodes...")

    # R√©cup√©ration des IDs des √©pisodes
    cursor.execute("SELECT id, duration_minutes FROM episodes LIMIT 1000")
    episodes = cursor.fetchall()

    # R√©cup√©ration des IDs utilisateurs
    cursor.execute("SELECT id FROM users WHERE is_active = TRUE LIMIT 2000")
    user_ids = [row[0] for row in cursor.fetchall()]

    # R√©cup√©ration des IDs de sessions de visionnage pour les s√©ries
    cursor.execute("""
        SELECT vs.id, vs.user_id 
        FROM viewing_sessions vs
        JOIN content c ON vs.content_id = c.id
        WHERE c.content_type = 'tv_show'
        LIMIT 5000
    """)
    viewing_sessions_data = cursor.fetchall()

    episode_viewing_data = []

    # Progress bar
    pbar = tqdm(total=min(n_episode_views, 30000), desc="G√©n√©ration des visionnages d'√©pisodes", unit="view")

    for i in range(min(n_episode_views, 30000)):
        # Choisir une session de visionnage ou cr√©er une nouvelle
        if viewing_sessions_data and random.random() < 0.7:
            viewing_session_id, user_id = random.choice(viewing_sessions_data)
        else:
            viewing_session_id = None
            user_id = random.choice(user_ids)

        # Choisir un √©pisode
        episode_id, episode_duration = random.choice(episodes)

        # Heure de d√©but
        start_time = fake.date_time_between(
            start_date=datetime.now() - timedelta(days=90),
            end_date=datetime.now()
        )

        # Dur√©e regard√©e (entre 5 minutes et la dur√©e totale)
        max_watch = min(episode_duration * 60, 3600)  # Max 1 heure
        duration_watched = random.randint(300, max_watch)  # Min 5 minutes

        # Taux de compl√©tion
        completion_rate = min(duration_watched / (episode_duration * 60), 1.0) * 100

        viewing_record = (
            viewing_session_id, episode_id, user_id, start_time,
            start_time + timedelta(seconds=duration_watched),
            duration_watched, round(completion_rate, 2)
        )

        episode_viewing_data.append(viewing_record)

        # Insertion par lots de 1000
        if len(episode_viewing_data) >= 1000:
            query = """
                INSERT INTO episode_viewing (
                    viewing_session_id, episode_id, user_id, start_time,
                    end_time, duration_watched, completion_rate
                ) VALUES (%s, %s, %s, %s, %s, %s, %s)
            """
            execute_batch(cursor, query, episode_viewing_data)
            episode_viewing_data = []

        pbar.update(1)

    # Insertion des donn√©es restantes
    if episode_viewing_data:
        query = """
            INSERT INTO episode_viewing (
                viewing_session_id, episode_id, user_id, start_time,
                end_time, duration_watched, completion_rate
            ) VALUES (%s, %s, %s, %s, %s, %s, %s)
        """
        execute_batch(cursor, query, episode_viewing_data)

    pbar.close()
    conn.commit()
    cursor.close()
    logger.info(f"‚úÖ {min(n_episode_views, 30000)} visionnages d'√©pisodes g√©n√©r√©s avec succ√®s")


def verify_data(conn):
    """V√©rifie et affiche un r√©capitulatif des donn√©es g√©n√©r√©es"""
    logger.info("V√©rification des donn√©es g√©n√©r√©es...")

    cursor = conn.cursor(cursor_factory=RealDictCursor)

    # Requ√™tes de v√©rification
    queries = [
        ("Utilisateurs", "SELECT COUNT(*) as count FROM users"),
        ("Utilisateurs actifs", "SELECT COUNT(*) as count FROM users WHERE is_active = TRUE"),
        ("Contenus", "SELECT COUNT(*) as count FROM content"),
        ("Films", "SELECT COUNT(*) as count FROM content WHERE content_type = 'movie'"),
        ("S√©ries TV", "SELECT COUNT(*) as count FROM content WHERE content_type = 'tv_show'"),
        ("Documentaires", "SELECT COUNT(*) as count FROM content WHERE content_type = 'documentary'"),
        ("Sessions de visionnage", "SELECT COUNT(*) as count FROM viewing_sessions"),
        ("√âvaluations", "SELECT COUNT(*) as count FROM ratings"),
        ("Liste de visionnage", "SELECT COUNT(*) as count FROM watchlist"),
        ("√âv√©nements d'abonnement", "SELECT COUNT(*) as count FROM subscription_events"),
        ("Requ√™tes de recherche", "SELECT COUNT(*) as count FROM search_queries"),
        ("√âpisodes", "SELECT COUNT(*) as count FROM episodes"),
        ("Visionnages d'√©pisodes", "SELECT COUNT(*) as count FROM episode_viewing"),
    ]

    print("\n" + "=" * 60)
    print("R√âCAPITULATIF DES DONN√âES G√âN√âR√âES")
    print("=" * 60)

    total_records = 0
    for label, query in queries:
        cursor.execute(query)
        result = cursor.fetchone()
        count = result['count'] if result else 0
        total_records += count
        print(f"{label:30} : {count:>10,}")

    print("-" * 60)
    print(f"{'TOTAL':30} : {total_records:>10,}")
    print("=" * 60)

    # Quelques statistiques suppl√©mentaires
    print("\nüìä Statistiques suppl√©mentaires:")

    # Temps de visionnage total
    cursor.execute("SELECT SUM(duration_seconds) as total_watch_seconds FROM viewing_sessions")
    result = cursor.fetchone()
    total_seconds = result['total_watch_seconds'] or 0
    total_hours = total_seconds / 3600
    print(f"‚Ä¢ Temps total de visionnage : {total_hours:,.0f} heures")

    # Note moyenne
    cursor.execute("SELECT AVG(rating) as avg_rating FROM ratings")
    result = cursor.fetchone()
    avg_rating = result['avg_rating'] or 0
    print(f"‚Ä¢ Note moyenne : {avg_rating:.2f}/5")

    # Taux de compl√©tion moyen
    cursor.execute("SELECT AVG(completion_rate) as avg_completion FROM viewing_sessions")
    result = cursor.fetchone()
    avg_completion = result['avg_completion'] or 0
    print(f"‚Ä¢ Taux de compl√©tion moyen : {avg_completion:.1f}%")

    # R√©partition des abonnements
    cursor.execute("""
        SELECT subscription_plan, COUNT(*) as count,
               ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER(), 1) as percentage
        FROM users
        GROUP BY subscription_plan
        ORDER BY count DESC
    """)
    print("\nüìà R√©partition des plans d'abonnement:")
    for row in cursor.fetchall():
        print(f"  ‚Ä¢ {row['subscription_plan']:15} : {row['count']:>6,} ({row['percentage']}%)")

    cursor.close()


def main():
    """Point d'entr√©e principal"""
    print("\n" + "=" * 70)
    print("G√âN√âRATEUR DE DONN√âES STREAMVISION - PLATEFORME DE STREAMING")
    print("=" * 70)

    # Avertissement
    print("\n‚ö†Ô∏è  ATTENTION: Cette op√©ration va g√©n√©rer une grande quantit√© de donn√©es.")
    print("   Temps estim√©: 5-15 minutes selon votre machine.")

    try:
        # Connexion √† la base de donn√©es
        conn = get_db_connection()

        print("\n" + "-" * 70)
        print("QUELLES DONN√âES VOULEZ-VOUS G√âN√âRER ?")
        print("-" * 70)
        print("1. ‚úÖ Toutes les donn√©es (recommand√©)")
        print("2. ‚öôÔ∏è  Personnaliser la g√©n√©ration")
        print("3. üîç V√©rifier les donn√©es existantes")
        print("4. üßπ FLUSH DATABASE (SUPPRIMER TOUT)")
        print("5. üö™ Quitter")

        choice = input("\nVotre choix [1-4]: ").strip()

        if choice == "1":
            # G√©n√©ration compl√®te
            print("\nüé¨ D√©marrage de la g√©n√©ration compl√®te...")

            generate_users(conn, n_users=10000)
            generate_content(conn, n_content=5000)
            generate_viewing_sessions(conn, n_sessions=100000)
            generate_ratings(conn, n_ratings=30000)
            generate_watchlist(conn, n_items=20000)
            generate_subscription_events(conn, n_events=15000)
            generate_search_queries(conn, n_queries=25000)
            generate_episodes_and_viewing(conn, n_episodes=5000, n_episode_views=30000)

            verify_data(conn)

        elif choice == "2":
            # G√©n√©ration personnalis√©e
            print("\nüîß G√©n√©ration personnalis√©e")

            n_users = int(input("Nombre d'utilisateurs [10000]: ") or "10000")
            n_content = int(input("Nombre de contenus [5000]: ") or "5000")
            n_sessions = int(input("Nombre de sessions [100000]: ") or "100000")

            generate_users(conn, n_users)
            generate_content(conn, n_content)
            generate_viewing_sessions(conn, n_sessions)

            # Les autres tables sont optionnelles
            if input("\nG√©n√©rer les √©valuations? [O/n]: ").strip().lower() != 'n':
                generate_ratings(conn, n_ratings=30000)

            if input("G√©n√©rer la watchlist? [O/n]: ").strip().lower() != 'n':
                generate_watchlist(conn, n_items=20000)

            verify_data(conn)

        elif choice == "3":
            # V√©rification seulement
            verify_data(conn)

        elif choice == "4":
            flush_database(conn)

        elif choice == "5":
            print("\nAu revoir!")
            sys.exit(0)

        else:
            print("\n‚ùå Choix invalide. Sortie.")
            sys.exit(1)

        # Fermeture de la connexion
        conn.close()
        logger.info("Connexion PostgreSQL ferm√©e")

        print("\n" + "=" * 70)
        print("‚úÖ G√âN√âRATION TERMIN√âE AVEC SUCC√àS !")
        print("=" * 70)
        print("\nProchaines √©tapes:")
        print("1. ‚úÖ PostgreSQL est pr√™t avec des donn√©es r√©alistes")
        print("2. ‚û°Ô∏è  Passez √† l'export vers S3")
        print("\nüìÅ Les logs d√©taill√©s sont disponibles dans: data_generation.log")

    except KeyboardInterrupt:
        print("\n\n‚èπÔ∏è  G√©n√©ration interrompue par l'utilisateur.")
        sys.exit(0)
    except Exception as e:
        logger.error(f"Erreur fatale: {e}", exc_info=True)
        print(f"\n‚ùå ERREUR: {e}")
        print("Consultez le fichier data_generation.log pour plus de d√©tails.")
        sys.exit(1)


if __name__ == "__main__":
    main()